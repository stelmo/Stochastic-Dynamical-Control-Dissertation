\documentclass[../masters.tex]{subfiles}

\begin{document}
\graphicspath{{./imgs/}{../imgs/}} %look for images

\section{Linear Hybrid Models}
In this section we generalise the graphical models of the previous sections as shown in Figure \ref{fig_hybridmod1}. We include the discrete random variables, $s_1, s_2,...$ where each variable has $N$ states, which we will call the switching variables. The goal of adding switching variables is to allow our graphical models to switch (or more precisely, choose based on the observation) between $N$ different dynamical models. For the moment we restrict ourselves to linear transition functions i.e. we use linear state space models. The other variables retain their meaning as before.     
\begin{figure}[H] 
\centering
\includegraphics[scale=1.0]{hybrid_model.pdf}
\caption{Graphical model of this section}
\label{fig_hybridmod1}
\end{figure}
One of the benefits of combining discrete switching variables with linear dynamical models is that it allows us to model nonlinear processes with linear models. Intuitively, we can glue together linear models which each describe a nonlinear model in some region and use the switch to determine which one to use. The switch assigns a weight to each model based on its ability to explain the evidence. 

We model this system as follows. Let $s_t=1,2,..., N$ denote a discrete, time homogeneous $N$ state first order Markov chain with transition matrix $P$ as discussed in the Hidden Markov Model section. Let each state $s_t=i$ be associated with a parameter set $\left(A_i, B_i, b_i, Q_i, C_i, R_i \right)$ used to evaluate the dynamical model shown in (\ref{eq_smodel}). If $s_t$ were observed then (\ref{eq_smodel}) would simplify to a set of linear latent dynamical systems we could perform inference on using the methods investigated in the Linear Models section. However, we assume that $s_t$ is a hidden random variable.
\begin{equation}
\begin{aligned}
x_{t+1} &= A_ix_t + B_iu_t + b_i + w_{t+1} \text{ with } \mathcal{N}(w_{t+1}|0,Q_i) \\
y_{t+1} &= C_ix_{t+1} + v_{t+1}  \text{ with } \mathcal{N}(v_{t+1}|0,R_i)
\end{aligned}
\label{eq_smodel}
\end{equation}
To fully specify the system we require the prior distributions $p(s_1)$ and $p(x_1|s_1)$.

\subsection{Exact Inference}
The switching variables ($s_1, s_2,...$) are discrete random variables exactly like the ones seen in the Hidden Markov Model section. There we derived recursive analytic expressions for inference which were computationally inexpensive. The stochastic linear dynamical system ($x_1,x_2,...$ and $y_1, y_2,...$) is exactly the same as the section on Linear Models. There we derived the famous Kalman Filter equations which were also analytic, recursive and computationally inexpensive. Taking this into consideration it seems plausible to believe that inference, specifically filtering, for hybrid systems like (\ref{eq_smodel}) can be formulated in a computationally feasible manner. 

Unfortunately, it can be shown that this is not possible in general \cite{lerner}\cite{murphy3} because the memory requirements scale exponentially with time. Loosely speaking one can see this by noting that at the first time step the system is described by a weighted set of $N$ Kalman Filter models (due to the linear assumption and the $N$ switching indices). At time step two the system is described by a weighted set of $N^2$ Kalman Filter models. Continuing in this manner we see that at time step $t$ the memory requirement is $N^t$. Clearly this is computationally infeasible and calls for approximate methods to be used. 

In literature many approximate filtering algorithms exist and it is not clear which is best. Two of the more popular methods include Gaussian Sum Filtering \cite{barber2} and Particle Filtering based methods (specifically the Rao-Blackwellisation approach). Both of these methods take advantage of the Gaussian structure of the system and operate in a fixed memory space making them computationally attractive. We focus on the Particle based methods because it can be extended to nonlinear systems with ease.   

\subsection{Rao-Blackwellised Particle Filter}
It is our objective to find the joint posterior distribution $p(s_{1:t}, x_{1:t}|y_{1:t})$. This joint posterior admits filtering of Figure \ref{fig_hybridmod1} if we discard the trajectory and focus only in $s_t,x_t$. By the chain rule we immediately have that $p(s_{1:t}, x_{1:t}|y_{1:t}) = p(s_{1:t}|y_{1:t})p(x_{1:t}|y_{1:t}, s_{1:t})$. Given $s_{1:t}$ we see that $p(x_{1:t}|y_{1:t}, s_{1:t})$ can be evaluated using the Kalman Filter equations and thus we are only concerned with finding some approximation for $p(s_{1:t}|y_{1:t})$. This is the essence of the Rao-Blackwellised Particle Filter - taking advantage of the Gaussian nature of the system to analytically evaluate a part of the posterior distribution \cite{doucet}.

Using the formulation of the adaptive Sequential Importance Sampling algorithm discussed in the previous section we can apply it to find an approximation of $p(s_{1:t}|y_{1:t})$. We set $\gamma_t(s_{1:t})=p(s_{1:t},y_{1:t})$ and $Z_t=p(y_{1:t})$ and then have that $\frac{\gamma_t(s_{1:t})}{Z_t} = p(s_{1:t}|y_{1:t})$ as desired. We then choose our proposal distribution $q_t(s_{1:t}|y_{1:t})$ to be recursive and follow the same procedure as before shown in (\ref{eq_rbweight1}).
\begin{equation}
\begin{aligned}
w_t(s_{1:t}) &= \frac{\gamma_t(s_{1:t},y_{1:t})}{q_t(s_{1:t}|y_{1:t})} \\
&= \frac{p(s_{1:t},y_{1:t})}{q_t(s_{1:t}|y_{1:t})} \\
&\propto \frac{p(s_{1:t}|y_{1:t})}{q_t(s_{1:t}|y_{1:t})} \\
&\propto \frac{p(y_t|s_t)p(s_t|s_{t-1})}{q_t(s_t|s_{1:t-1},y_{1:t})}\frac{p(s_{1:t-1}|y_{1:t-1})}{q_t(s_{1:t-1}|y_{1:t-1})} \\
&= \alpha_t(s_{1:t})w_{t-1}(s_{1:t-1})
\end{aligned}
\label{eq_rbweight1}
\end{equation}
As before, we are not interested in the whole trajectory of the switching variable because we only need to perform filtering. Thus our proposal distribution can be chosen to be the prior i.e. $q_t(s_t|s_{1:t-1},y_{1:t}) = p(s_t|s_{t-1})$. This is suboptimal but easy to sample from \cite{doucet}. The incremental weight then simplifies to $\alpha_t(s_{1:t}) = p(y_t|s_t)$. We can evaluate this distribution by marginalising out $x_t$ \cite{murphy1} and using the properties of the Gaussian distributions as before (\ref{eq_rbweight2}).
\begin{equation}
\begin{aligned}
\alpha_t(s_{1:t}) &= p(y_t|s_t) \\
&= \int_{x_t} p(y_t|x_t,s_t)p(x_t|s_{1:t},y_{1:t-1}) \\
&= p(y_t|y_{1:t-1}, s_{1:t}) \\
&= \mathcal{N}\left(y_t | C_{s_t}A_{s_t}\mu_{t-1}, C_{s_t}\left(A_{s_t}\Sigma_{t-1}A_{s_t}^T+Q_{s_t} \right)C_{s_t} + R_{s_t} \right)
\end{aligned}
\label{eq_rbweight2}
\end{equation} 
Where $s_t$ is denotes a specific state of the switching variable. Upon inspection we see that (\ref{eq_rbweight2}) is just the one step ahead prediction likelihood as discussed in the Linear Models section on prediction \cite{murphy1}. Note that we will still need to resample $s_t$ from $P$ periodically to prevent sample impoverishment. 

We now have an efficient particle approximation of $p(s_t|y_t)$. To find the filtered posterior distribution as desired we note that $p(s_t,x_t|y_{1:t}) = \sum_i w_t^i(S_t^i)\delta(S_t^i, s_t)p(x_t|y_{1:t}, S_t^i)$ where $S_t^i$ is the i$^{\text{th}}$ particle. Each particle thus consists of a weight, a switch sample and the sufficient statistics generated by the Kalman Filter for a Gaussian i.e. a mean and covariance. The complete algorithm is shown below.

\textbf{Rao-Blackwellised Particle Filter Algorithm}\\
For $t=1$:
\begin{enumerate}
\item
Sample $S^i_1 \backsim p(s_1)$ and $\mu_{1|0}^i \backsim p(x_1|s_1)$.
\item
Compute the weights $w_1(S_1^i) = p(Y^*_1|S_1^i)$ where $Y^*_1$ is the observation. Normalise $W^i_1 \propto w_1(S^i_1)$.
\item
Apply the update step of the Kalman Filter to each particle $i$ and associated parameters to find $\mu_1^i$ and $\Sigma_1^i$. 
\item
If the number of effective particles is below some threshold apply resampling with roughening $(W^i_1, {S}^i_1,{\mu}^i_1, {\Sigma}^i_1)$ to obtain $N$ equally weighted particles $(\frac{1}{N}, \bar{S}^i_1, \bar{\mu}^i_1, \bar{\Sigma}^i_1)$ and set $(\bar{W}^i_1, \bar{S}^i_1, \bar{\mu}^i_1, \bar{\Sigma}^i_1) \leftarrow (\frac{1}{N}, \bar{S}^i_1, \bar{\mu}^i_1, \bar{\Sigma}^i_1)$ otherwise set $(\bar{W}^i_1, \bar{S}^i_1, \bar{\mu}^i_1, \bar{\Sigma}^i_1) \leftarrow ({W}^i_1, {S}^i_1, \mu^i_1, \Sigma_1^i)$
\end{enumerate}
For $t \geq 2$:
\begin{enumerate}
\item
Sample $S^i_t \backsim p(S_t^i|\bar{S}^i_{t-1})$.
\item
Compute the weights $\alpha_t(S^i_{t}) = p(Y^*_t|S_t^i)$ and normalise $W^i_t \propto \bar{W}^i_{t-1}\alpha_t(S^i_{t})$.
\item
Apply the Kalman Filter algorithm to $\mu_{t-1}$ and $\Sigma_{t-1}$ for each particle $i$ to find the sufficient statistics $\mu_{t}$ and $\Sigma_{t}$ using the parameters corresponding to the state of $S^i_t$.
\item
If the number of effective particles is below some threshold apply resampling with roughening $(W^i_t, {S}^i_t,{\mu}^i_t, {\Sigma}^i_t)$ to obtain $N$ equally weighted particles $(\frac{1}{N}, \bar{S}^i_t, \bar{\mu}^i_t, \bar{\Sigma}^i_t)$ and set $(\bar{W}^i_t, \bar{S}^i_t, \bar{\mu}^i_t, \bar{\Sigma}^i_t) \leftarrow (\frac{1}{N}, \bar{S}^i_t, \bar{\mu}^i_t, \bar{\Sigma}^i_t)$ otherwise set $(\bar{W}^i_t, \bar{S}^i_t, \bar{\mu}^i_t, \bar{\Sigma}^i_t) \leftarrow ({W}^i_t, {S}^i_t, \mu^i_t, \Sigma_1^t)$
\end{enumerate} 

\subsection{Rao-Blackwellised Particle Prediction}
Like the Particle Predictor studied in the previous section, performing prediction using Rao-Blackwellisation is straightforward because there is no weighting step. Each particle's switching state is merely propogated forward using the transition matrix $P$ and the Kalman prediction algorithm is used to evaluate the predicted mean and covariance.

\subsection{Filter the CSTR}

\subsection{Controlling the CSTR}


\bibliographystyle{plain}
\bibliography{research}

\end{document}