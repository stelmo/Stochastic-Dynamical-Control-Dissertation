\documentclass[../masters.tex]{subfiles}

\begin{document}
\section{Literature}
Insert an introduction and overview of the literature study.

\subsection{Probability Theory}
The calculus of Probability Theory was developed by Fermat and Pascal in order to better understand the problems introduced by uncertainty in gambling. From this dubious genesis a rich and incredibly powerful field has developed. We start our brief introduction of probability theory by stating Kolmogorov's three probability axioms - these axioms underpin the entire theory of probability.

Let $U$ be the universe of possible events, also called an event space; that is, if we are uncertain about which of a number of possibilities are true then we let U represent all of them collectively. Let $P$ be some function which satisfies the three axioms stated below.
\begin{ax}
$P(U) = 1$. The probability of any event in $U$ occurring is 1.
\end{ax}
\begin{ax}
$\forall X \subseteq U,~P(X) \geq 0$. Let $X$ be some event or collection of events in $U$ with the possibility that $X$ is empty not excluded. The probability of $X$ occurring is non-negative.
\end{ax}
\begin{ax}
$\forall X, Y \subseteq U,\text{ if } X \cap Y = \emptyset \text{ then } P(X \cup Y) = P(X) + P(Y)$. The addition of two disjoint event sets in $U$ is equal to the sum of their probabilities.
\end{ax}
A function $P$ which satisfies these three axioms is known as a probability function. Based on these three axioms we are able to extend the theory to Theorem \ref{thrm_prob_union_intersect}.
\begin{thrm}
$\forall X, Y \subseteq U, P(X \cup Y) = P(X) + P(Y) + P(X \cap Y)$
\label{thrm_prob_union_intersect}
\end{thrm}
Before we proceed let us briefly discuss how we can interpret the function $P$ given some event $X$. If $P(X)=1$ we are certain of event $X$ occurring; likewise, if $P(X)=0$ we are certain that event $X$ will not occur. Thus our certainty of event $X$ occurring is reflected by the magnitude of $P(X)$. Attempting to make the statement ``our certainty of event $X$ occurring" more precise leads us to two different physical interpretations of $P(X)$. The first is the frequentist interpretation: to the frequentist a probability is a long term average of the observations of event $X$ occurring in the event space. While this interpretation is satisfying if one deals with something which is easily measured e.g. the probability of a fair die rolling a 6 it fails to explain statements like: ``the probability of it raining tomorrow is 50\%". The reason the last statement is problematic is because the time span is ill defined. If we rather consider probabilities as subjective degrees of belief in event $X$ occurring this is no longer a problem. To ensure that these subjective beliefs are rational can be problematic. One way to ensure this is by requiring that if the probabilities were used in a betting game it is impossible to exploit them to one's advantage (or disadvantage). 

It is now necessary to define what we mean by conditional probability. Definition \ref{defn_cond_prob} makes precise how the knowledge that event $Y$ has occurred alters our view of event $X$ occurring.  
\begin{defn}
\textbf{Conditional Probability} $P(X|Y) = \frac{P(X\cap Y)}{P(Y)}$ 
\label{defn_cond_prob}
\end{defn}
Note that if $P(Y) = 0$ then Definition \ref{defn_cond_prob} is undefined. Additionally, the function $P(\cdot|Y)$ is a probability function. We next define what we mean by a positive probability distribution in Definition \ref{defn_pos_distr} 
\begin{defn}
A probability distribution is positive if $P(X) > 0~\forall~X \in U$.
\label{defn_pos_distr}
\end{defn}
Clearly undefined conditional probabilities are not a problem in the setting of positive probability distributions. We also define the notion of independence, also sometimes called marginal independence, in Definition \ref{defn_indep}. Intuitively two events $X$ and $Y$ are independent if the outcome of $X$ does not influence the outcome of $Y$. It can be shown that independence is a symmetric property.
\begin{defn}
\textbf{Independence} $X \indep Y \equiv P(X|Y) = P(X)$ 
\label{defn_indep}
\end{defn}
Generalising the concept of independence we define conditional independence by Definition \ref{defn_cond_indep}. Again this definition is symmetric.
\begin{defn}
\textbf{Conditional Independence} $X \indep Y | Z \equiv P(X|Y,Z) = P(X|Z)$
\label{defn_cond_indep}
\end{defn}
Intuitively, if $X$ is conditionally independent of $Y$ given $Z$ then by knowing $Z$ one gains nothing by observing $Y$. Clearly if $Z=\emptyset$ we have (marginal) independence.
We introduce, without proof Theorem \ref{thrm_total_prob}.
\begin{thrm}
\textbf{Total Probability} Assume the set of events $\{A_i\}$ is a partition of $U$ and for any $i \neq j$ we have $A_i \cap A_j = \emptyset$ then $P(U) = \sum_iP(A_i)$.
\label{thrm_total_prob}
\end{thrm}
We also introduce Theorem \ref{thrm_chain_rule} which naturally leads us to the formulation of Bayes' Theorem (using Definition \ref{defn_cond_prob}) as shown in Theorem \ref{thrm_bayes}. 
\begin{thrm}
\label{thrm_chain_rule}  
\textbf{Chain Rule} Given $A_1, A_2 \in U$ we have $P(A_1\cap A_2) = P(A_1)P(A_2|A_1)$. Generalising we have that $P(A_1\cap A_2\cap ... \cap A_n) = P(A_1)P(A_2|A_1)...P(A_n|A_1\cap A_2 \cap ... \cap A_n)$.
\end{thrm}
\begin{thrm}
\textbf{Bayes' Theorem} $P(h|e) = \frac{P(e|h)P(h)}{P(e)}$
\label{thrm_bayes}
\end{thrm}
Under the Bayesian interpretation of Theorem \ref{thrm_bayes} we see that the posterior probability of some hypothesis $h$ given some evidence $e$ being true is just the likelihood $P(e|h)$ of the hypothesis supporting the evidence multiplied by the prior probability of the hypothesis $P(h)$ normalised by the prior of the evidence $P(e)$. 

We now make precise what we mean by random variables: a random variable is a non-deterministic variable which is characterised by some uncertainty in its measurement. So far in our discussion we have implicitly only used discrete variables, that is our probability space consisted out of a finite number of events or states. However, it is also necessary to make precise what we mean by a continuous random variable. A continuous random variable is characterised by a density function $f$ which assigns a weight to each possible value of the variable. Although the density function is itself not a probability function, if it satisfies $f(x) \geq 0~\forall x$ and $\int_{-\infty}^{+\infty} f(x)dx = 1$ then it can be used to generate one. The cumulative probability function $P(X \leq x)=\int_{x\prime \leq x}f(x\prime)dx\prime$ is one such example. 

We will also deal extensively with joint and marginal probability distributions. Consider the random variables $X$ and $Y$. The marginal probability distribution of $X$ is the function $P(X)$ and describes the probabilities of events involving only the variable $X$. The joint probability distribution of $X$ and $Y$ is the function $P(X,Y) = P(X \cup Y)$ and describes the union of the probability space of $X$ and $Y$. Using the Total Probability Theorem (Theorem \ref{thrm_total_prob}) we can reduce any joint distribution to a marginal one by summing (or integrating) out the appropriate variable: $P(Y) = \sum_x P(x, Y)$. 

Now that we know more about joint probability distributions it is pertinent to investigate common questions one can ask of a given joint distribution. The first is standard probability query: given some evidence $e$, what is the posterior probability of our hypothesis being true? This is formulated by $P(h|e)$ and can be answered using straightforward application of Theorem \ref{thrm_bayes}. Another practically useful query is the MAP query. This query finds the most likely hypothesis which explains the observed evidence: $\text{MAP}(h|e)=\underset{h}{\text{arg max}} P(h,e) $

Finally, it is also useful to define what we mean by an expected utility function. The probability weighted average of a set of outcomes defined by the utility function $P(O_i|A)$ given some action is used in Definition \ref{defn_expected_utility}.
\begin{defn}
\textbf{Expected Utility} $EU(A) = \sum_i U(O_i|A) \times P(O_i|A)$
\label{defn_expected_utility}
\end{defn}
It is common for Bayesian agents to attempt to maximise their expected utility by some course of actions.

\subsection{Bayes' Theorem: Application}
This section will attempt to develop some intuition behind Theorem \ref{thrm_bayes}. We quote an excerpt from an Article in the Economist \cite{eco1} and illustrate the use of Bayes' Rule in a canonical medical example.

\textit{``The essence of the Bayesian approach is to provide a mathematical rule explaining how you should change your existing beliefs in the light of new evidence. In other words, it allows scientists to combine new data with their existing knowledge or expertise. The canonical example is to imagine that a precocious newborn observes his first sunset, and wonders whether the sun will rise again or not. He assigns equal prior probabilities to both possible outcomes, and represents this by placing one white and one black marble into a bag. The following day, when the sun rises, the child places another white marble in the bag. The probability that a marble plucked randomly from the bag will be white (ie, the child's degree of belief in future sunrises) has thus gone from a half to two-thirds. After sunrise the next day, the child adds another white marble, and the probability (and thus the degree of belief) goes from two-thirds to three-quarters. And so on. Gradually, the initial belief that the sun is just as likely as not to rise each morning is modified to become a near-certainty that the sun will always rise."}


\subsection{Bayesian Networks}

\subsubsection{d-separation}


\subsection{Dynamic Bayesian Networks}
Chapter 13 in Bishop! HMM and then KFs

\end{document}